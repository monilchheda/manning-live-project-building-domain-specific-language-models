{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character LM with AllenNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monilchheda/manning-live-project-building-domain-specific-language-models/blob/master/Character_LM_with_AllenNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqdnocVYwo2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Wyv1jTwtRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the dataset\n",
        "df_raw = pd.read_csv('https://alexip-ml.s3.amazonaws.com/stackexchange_812k.tokenized.csv.gz', compression='gzip').sample(frac = 1, random_state = 8).reset_index(drop = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ovLwiXH1kXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = ''.join(df_raw.text.values).lower()\n",
        "\n",
        "# simple but efficient way to split string into list of characters\n",
        "all_characters = [s for s in text]\n",
        "unique_characters = np.unique(all_characters) \n",
        "print(unique_characters)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yCqy4Io9b1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "char_count = Counter(all_characters)\n",
        "print(char_count.most_common())\n",
        "\n",
        "# limit the allowed characters to MAX_VOCAB_SIZE\n",
        "MAX_VOCAB_SIZE = 40\n",
        "valid_characters = [t[0] for t in  char_count.most_common(MAX_VOCAB_SIZE)]\n",
        "valid_characters.sort()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuQwpa2z1s66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "POSTS_TYPE = 'title'\n",
        "DF_SAMPLE_COUNT = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZNgVbmc-6V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# subsample the original dataset\n",
        "\n",
        "df = df_raw[(df_raw.category == POSTS_TYPE)].sample(DF_SAMPLE_COUNT).reset_index(drop = True)\n",
        "\n",
        "print(\"df.shape: \", df.shape)\n",
        "\n",
        "print(df.text.sample(2).values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll4Atnsu1B6h",
        "colab_type": "text"
      },
      "source": [
        "Install AllenNLP  if needed (by uncommenting)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saNTiSdT1COh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4-VorOX1WkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from typing import Dict, List, Tuple, Set\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
        "from allennlp.data.fields import TextField\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.iterators import BasicIterator\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token, CharacterTokenizer\n",
        "from allennlp.data.vocabulary import Vocabulary, DEFAULT_PADDING_TOKEN\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.trainer import Trainer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ili0K03oBUOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = CharacterTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EO44rWKDY1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = df.text.apply(lambda txt : tokenizer.tokenize(txt.lower())).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oh8HXNU2FWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tokens_to_instance(tokens: List[Token], token_indexers: Dict[str, TokenIndexer]):\n",
        "    tokens = list(tokens)\n",
        "    tokens.insert(0, Token(START_SYMBOL))\n",
        "    tokens.append(Token(END_SYMBOL))\n",
        "\n",
        "    input_field  = TextField(tokens[:-1], token_indexers)\n",
        "    output_field = TextField(tokens[1:], token_indexers)\n",
        "    return Instance({'input_tokens': input_field, 'output_tokens': output_field})        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPwuYbUhDczN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
        "instances = [tokens_to_instance(tokens, token_indexers) for tokens in train_set]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHP28UrPhTbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_counts = {char: 1 for char in valid_characters}\n",
        "vocab = Vocabulary({'tokens': token_counts})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR3fq534FfN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = 32\n",
        "HIDDEN_SIZE = 256\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8bkt-6EHsv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNLanguageModel(Model):\n",
        "    def __init__(self,\n",
        "                 embedder: TextFieldEmbedder,\n",
        "                 hidden_size: int,\n",
        "                 max_len: int,\n",
        "                 vocab: Vocabulary) -> None:\n",
        "        super().__init__(vocab)\n",
        "\n",
        "        self.embedder = embedder\n",
        "\n",
        "        # initialize a Seq2Seq encoder, LSTM\n",
        "        self.rnn = PytorchSeq2SeqWrapper(\n",
        "            torch.nn.LSTM(EMBEDDING_SIZE, HIDDEN_SIZE, batch_first=True))\n",
        "\n",
        "        self.hidden2out = torch.nn.Linear(in_features=self.rnn.get_output_dim(), out_features=vocab.get_vocab_size('tokens'))\n",
        "        self.hidden_size = hidden_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, input_tokens, output_tokens):\n",
        "\n",
        "        mask = get_text_field_mask(input_tokens)\n",
        "        embeddings = self.embedder(input_tokens)\n",
        "        rnn_hidden = self.rnn(embeddings, mask)\n",
        "        out_logits = self.hidden2out(rnn_hidden)\n",
        "        loss = sequence_cross_entropy_with_logits(out_logits, output_tokens['tokens'], mask)\n",
        "\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def generate(self) -> Tuple[List[Token], torch.tensor]:\n",
        "\n",
        "        start_symbol_idx = self.vocab.get_token_index(START_SYMBOL, 'tokens')\n",
        "        end_symbol_idx = self.vocab.get_token_index(END_SYMBOL, 'tokens')\n",
        "        padding_symbol_idx = self.vocab.get_token_index(DEFAULT_PADDING_TOKEN, 'tokens')\n",
        "\n",
        "        log_likelihood = 0.\n",
        "        words = []\n",
        "        state = (torch.zeros(1, 1, self.hidden_size), torch.zeros(1, 1, self.hidden_size))\n",
        "\n",
        "        word_idx = start_symbol_idx\n",
        "\n",
        "        for i in range(self.max_len):\n",
        "            tokens = torch.tensor([[word_idx]])\n",
        "\n",
        "            embeddings = self.embedder({'tokens': tokens})\n",
        "            output, state = self.rnn._module(embeddings, state)\n",
        "            output = self.hidden2out(output)\n",
        "\n",
        "            log_prob = torch.log_softmax(output[0, 0], dim=0)\n",
        "\n",
        "            dist = torch.exp(log_prob)\n",
        "\n",
        "            word_idx = start_symbol_idx\n",
        "\n",
        "            while word_idx in {start_symbol_idx, padding_symbol_idx}:\n",
        "                word_idx = torch.multinomial(\n",
        "                    dist, num_samples=1, replacement=False).item()\n",
        "\n",
        "            log_likelihood += log_prob[word_idx]\n",
        "\n",
        "            if word_idx == end_symbol_idx:\n",
        "                break\n",
        "\n",
        "            token = Token(text=self.vocab.get_token_from_index(word_idx, 'tokens'))\n",
        "            words.append(token)\n",
        "\n",
        "        return words, log_likelihood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXsr-q6FGE25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=EMBEDDING_SIZE)\n",
        "\n",
        "embedder = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "\n",
        "model = RNNLanguageModel(embedder=embedder, hidden_size=HIDDEN_SIZE, max_len=80, vocab=vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5sScnaXGKPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator = BasicIterator(batch_size=BATCH_SIZE)\n",
        "iterator.index_with(vocab)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=5.e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv4-YO8lGOVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=instances,\n",
        "                  num_epochs=5)\n",
        "trainer.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGL-MrZpH1ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(text: str, model: Model) -> float:\n",
        "    tokenizer = CharacterTokenizer()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    \n",
        "    token_indexers = {'tokens': SingleIdTokenIndexer()}\n",
        "    instance = tokens_to_instance(tokens, token_indexers)\n",
        "    output = model.forward_on_instance(instance)\n",
        "    print(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHNyRLUiMMfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"In a fixed-effects model only time-varying variables can be used.\"\n",
        "predict(sentence, model)\n",
        "\n",
        "sentence = \"I know a pretty little place in Southern California, down San Diego way.\"\n",
        "predict(sentence, model)\n",
        "\n",
        "sentence = \"This that is noon but yes apple whatever did regression variable\"\n",
        "predict(sentence, model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpINqF64MP5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(50):\n",
        "    tokens, _ = model.generate()\n",
        "    print(''.join(token.text for token in tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}