{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "building-domain-specific-language-models-1-preparing-dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEswITh6eAYoAobiUXZU6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monilchheda/manning-live-project-building-domain-specific-language-models/blob/master/building_domain_specific_language_models_1_preparing_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lMjxi05EFUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tk = WordPunctTokenizer() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UzAtynaEZHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_it(txt):\n",
        "    #print(type(txt))\n",
        "    # I am sure there is a better way to handle this, but, here goes nothing :-D\n",
        "\n",
        "    #need to convert the input (which is list type) to string, by adding ''\n",
        "    original = ''.join(txt)\n",
        "    \n",
        "    # remove extra $$ - could not figure out a way to remove with regx below for latex\n",
        "    no_multiple_dollar = original.replace(\"$$\",\"$\")\n",
        "    \n",
        "    # BeautifulSoup to clean up majority html tags\n",
        "    bs_clean = BeautifulSoup(no_multiple_dollar, 'html.parser').get_text()\n",
        "    \n",
        "    # Stray links need to be removed\n",
        "    url_pattern = r'https?:\\/\\/.*'\n",
        "    url_pattern_cleaned = re.sub(url_pattern , \"\", bs_clean)\n",
        "    \n",
        "    # remove most latex patterns, some strays like postid 239921 not being cleaned... strange\n",
        "    latex_pattern = r'(?<!\\\\)\\$.*?(?<!\\\\)\\$'\n",
        "    latex_pattern_cleaned = re.sub(latex_pattern , \"\", url_pattern_cleaned)\n",
        "    \n",
        "    # remove carriage returns - no suitable example found, in the blind here\n",
        "    carriage_return_pattern = r'(?<![\\r\\n])(?:\\r\\n|\\r|\\n){2}[^\\r\\n]+'\n",
        "    carriage_return_cleaned = re.sub(carriage_return_pattern , \"\", latex_pattern_cleaned)\n",
        "    \n",
        "    # remove all digits - 240019\n",
        "    number_pattern = r'[0-9]+'\n",
        "    number_pattern_cleaned = re.sub(number_pattern , \"\", carriage_return_cleaned)\n",
        "    \n",
        "    # remove usernames/ twitter handles - 240019\n",
        "    user_quote_pattern = r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)'\n",
        "    user_quote_cleaned = re.sub(user_quote_pattern , \"\", number_pattern_cleaned)\n",
        "    \n",
        "\n",
        "    # brackets\n",
        "    #    brackets_pattern = r' ?\\([^)]+\\)'\n",
        "    round_brackets_pattern = r'[\\(\\)]'\n",
        "    round_brackets_pattern_clean = re.sub(round_brackets_pattern , \"\", user_quote_cleaned)\n",
        "    \n",
        "    # square brackets\n",
        "    square_brackets_pattern = r'[\\[\\]]'\n",
        "    square_brackets_pattern_clean = re.sub(square_brackets_pattern, \"\", round_brackets_pattern_clean)\n",
        "    \n",
        "    # Curly brackets\n",
        "    curly_brackets_pattern = r'[\\{\\}]'\n",
        "    curly_brackets_pattern_clean = re.sub(curly_brackets_pattern, \"\", square_brackets_pattern_clean)\n",
        "    \n",
        "    # Asterisk\n",
        "    asterisk_brackets_pattern = r'[\\*]'\n",
        "    asterisk_brackets_pattern_clean = re.sub(asterisk_brackets_pattern, \"\", curly_brackets_pattern_clean)\n",
        "\n",
        "    # remove almost anything not a word? - Maybe too strong.. https://stackoverflow.com/questions/875968/how-to-remove-symbols-from-a-string-with-python \n",
        "    #misc_cleanup_pattern = r'[^\\w]'\n",
        "    #misc_pattern_cleaned = re.sub(misc_cleanup_pattern, \" \", user_quote_cleaned)\n",
        "    \n",
        "    ## No clue for below items:\n",
        "    # 3. Remove missing values for texts\n",
        "    # 4. Remove texts that are extremely large or too short to bring any information to the model. We want to keep paragraphs that contain at least a few words and remove the paragraphs that are composed of large numerical tables.\n",
        "    # emojis etc\n",
        "    \n",
        "    \n",
        "    result = asterisk_brackets_pattern_clean\n",
        "    \n",
        "    tok_result = tk.tokenize(result.lower())\n",
        "    \n",
        "    return tok_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwdFVQrWEb4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://liveproject-resources.s3.amazonaws.com/116/other/stackexchange_812k.csv.gz\n",
        "#import os\n",
        "#print(os.getcwd())\n",
        "\n",
        "!gunzip stackexchange_812k.csv.gz\n",
        "\n",
        "df_full = pd.read_csv('stackexchange_812k.csv')\n",
        "df_full.info()\n",
        "df_full['cleaned_tokenized'] = df_full['text'].apply(clean_it)\n",
        "print(df_full.head())\n",
        "df_full.to_csv('stackexchange_812k_cleaned_tokenized.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}